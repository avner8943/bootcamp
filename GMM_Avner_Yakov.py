# -*- coding: utf-8 -*-
"""Welcome To Colaboratory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/avner8943/ml/blob/master/GMM.ipynb
"""

import matplotlib.pyplot as plt
#from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets
import numpy as np
from scipy.stats import multivariate_normal
from math import pi




def multi_gauss(data,means,cov): 

    
    res2 = multivariate_normal.pdf(data,means,cov)
    return res2

def EM_(X,k_,eps,iters):
    
    init_idx = np.random.choice(X.shape[0],k_,replace= False)
    
    means = X[init_idx,:]
    
       
    
    covs = [np.eye(X.shape[1])] * k_
    
    phi = np.zeros((k_,1))
    
    phi[:] = 1/k_
    
    memb_mat = np.zeros((X.shape[0],k_))
    

    
    log_likelihood = 10**10 # for first iteration
    
    thr = 1.0 # threshold for convergence
    
    iter = 0
    
    while (iter < iters and thr > eps):
        
        iter += 1
        print(iter)
        
        for k in range(k_):
    
            memb_mat[:,k] = phi[k]*multivariate_normal.pdf(X,means[k,:],covs[k]) # E step
        
        
        new_log_likelihood = np.sum(np.log(np.sum(memb_mat, axis = 1)))
        print(new_log_likelihood)
        
        thr = np.abs(log_likelihood - new_log_likelihood)
        
        log_likelihood = new_log_likelihood
        
        memb_mat = (memb_mat.T / np.sum(memb_mat, axis = 1)).T #normalization
        
        for k in range(k_): # M step
        
            phi[k] = np.mean(memb_mat[:,k])
        
            means[k,:] = np.sum(np.multiply(memb_mat[:,k].reshape(150,1),X), axis=0)/np.sum(memb_mat[:,k])
        
            covs[k] = (np.multiply(memb_mat[:,k], (X-means[k,:]).T)@(X-means[k,:]))/np.sum(memb_mat[:,k])

    
    
if __name__ == "__main__":
    
   
    iris = datasets.load_iris()
    X = iris.data
    

    eps = 0.000001
    
    iters = 1000
    
    k = 3
    
    EM_(X,k,eps,iters)
    
    